{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSPU6gneZTyZ",
        "outputId": "2ad0a4b3-6d99-41a2-f2c3-927d8932aa44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Reward Values: [0.42857143 0.42857143 0.53846154 0.68144691 0.11538462]\n",
            "Total Reward: 640\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "reward_history = []\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    reward_history.append(reward)\n",
        "\n",
        "print(\"Estimated Reward Values:\", estimates)\n",
        "\n",
        "total_reward = np.sum(reward_history)\n",
        "print(\"Total Reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "cumulative_reward = 0\n",
        "cumulative_mean_rewards = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    cumulative_reward += reward\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "print(\"Cumulative Mean Rewards:\", cumulative_mean_rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkCO72EOZXoA",
        "outputId": "115fd7d4-ceec-4dbd-aeb0-5d99752dce32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Mean Rewards: [1.0, 0.5, 0.3333333333333333, 0.5, 0.6, 0.6666666666666666, 0.7142857142857143, 0.625, 0.6666666666666666, 0.6, 0.5454545454545454, 0.5833333333333334, 0.5384615384615384, 0.5, 0.5333333333333333, 0.5, 0.47058823529411764, 0.5, 0.47368421052631576, 0.5, 0.5238095238095238, 0.5454545454545454, 0.5652173913043478, 0.5416666666666666, 0.52, 0.5384615384615384, 0.5185185185185185, 0.5357142857142857, 0.5517241379310345, 0.5666666666666667, 0.5483870967741935, 0.53125, 0.5454545454545454, 0.5588235294117647, 0.5714285714285714, 0.5833333333333334, 0.5945945945945946, 0.6052631578947368, 0.6153846153846154, 0.625, 0.6341463414634146, 0.6428571428571429, 0.627906976744186, 0.6363636363636364, 0.6222222222222222, 0.6304347826086957, 0.6170212765957447, 0.625, 0.6326530612244898, 0.62, 0.6274509803921569, 0.6153846153846154, 0.6226415094339622, 0.6111111111111112, 0.6181818181818182, 0.625, 0.6140350877192983, 0.6206896551724138, 0.6271186440677966, 0.6333333333333333, 0.639344262295082, 0.6290322580645161, 0.6190476190476191, 0.625, 0.6153846153846154, 0.6212121212121212, 0.6268656716417911, 0.6323529411764706, 0.6231884057971014, 0.6285714285714286, 0.6338028169014085, 0.6388888888888888, 0.6301369863013698, 0.6351351351351351, 0.64, 0.631578947368421, 0.6233766233766234, 0.6282051282051282, 0.6329113924050633, 0.6375, 0.6296296296296297, 0.6341463414634146, 0.6385542168674698, 0.6309523809523809, 0.6352941176470588, 0.6395348837209303, 0.632183908045977, 0.6363636363636364, 0.6404494382022472, 0.6333333333333333, 0.6373626373626373, 0.6413043478260869, 0.6451612903225806, 0.6382978723404256, 0.631578947368421, 0.625, 0.6288659793814433, 0.6224489795918368, 0.6262626262626263, 0.63, 0.6237623762376238, 0.6274509803921569, 0.6310679611650486, 0.6346153846153846, 0.638095238095238, 0.6320754716981132, 0.6355140186915887, 0.6388888888888888, 0.6422018348623854, 0.6454545454545455, 0.6486486486486487, 0.6517857142857143, 0.6548672566371682, 0.6578947368421053, 0.6521739130434783, 0.6551724137931034, 0.6581196581196581, 0.6610169491525424, 0.6554621848739496, 0.65, 0.6528925619834711, 0.6475409836065574, 0.6504065040650406, 0.6532258064516129, 0.656, 0.6587301587301587, 0.6614173228346457, 0.65625, 0.6589147286821705, 0.6615384615384615, 0.6564885496183206, 0.6590909090909091, 0.6616541353383458, 0.664179104477612, 0.6666666666666666, 0.6691176470588235, 0.6715328467153284, 0.6739130434782609, 0.6762589928057554, 0.6785714285714286, 0.6737588652482269, 0.676056338028169, 0.6783216783216783, 0.6736111111111112, 0.6689655172413793, 0.6712328767123288, 0.673469387755102, 0.6756756756756757, 0.6711409395973155, 0.6733333333333333, 0.6688741721854304, 0.6710526315789473, 0.673202614379085, 0.6753246753246753, 0.6774193548387096, 0.6794871794871795, 0.6815286624203821, 0.6772151898734177, 0.6792452830188679, 0.68125, 0.6770186335403726, 0.6728395061728395, 0.6748466257668712, 0.6707317073170732, 0.6727272727272727, 0.6686746987951807, 0.6646706586826348, 0.6666666666666666, 0.6627218934911243, 0.6647058823529411, 0.6666666666666666, 0.6627906976744186, 0.6647398843930635, 0.6666666666666666, 0.6628571428571428, 0.6647727272727273, 0.6666666666666666, 0.6685393258426966, 0.6703910614525139, 0.6666666666666666, 0.6629834254143646, 0.6593406593406593, 0.6612021857923497, 0.657608695652174, 0.6594594594594595, 0.6559139784946236, 0.6577540106951871, 0.6542553191489362, 0.6507936507936508, 0.6526315789473685, 0.6544502617801047, 0.6510416666666666, 0.6476683937823834, 0.6494845360824743, 0.6512820512820513, 0.6530612244897959, 0.649746192893401, 0.6464646464646465, 0.6482412060301508, 0.645, 0.6417910447761194, 0.6435643564356436, 0.6403940886699507, 0.6372549019607843, 0.6341463414634146, 0.6359223300970874, 0.6328502415458938, 0.6298076923076923, 0.631578947368421, 0.6333333333333333, 0.6350710900473934, 0.6367924528301887, 0.6384976525821596, 0.6355140186915887, 0.6325581395348837, 0.6342592592592593, 0.6359447004608295, 0.6330275229357798, 0.6301369863013698, 0.6318181818181818, 0.6334841628959276, 0.6351351351351351, 0.6322869955156951, 0.6339285714285714, 0.6311111111111111, 0.6327433628318584, 0.6299559471365639, 0.631578947368421, 0.62882096069869, 0.6304347826086957, 0.6320346320346321, 0.6336206896551724, 0.6351931330472103, 0.6324786324786325, 0.6340425531914894, 0.635593220338983, 0.6329113924050633, 0.6302521008403361, 0.6317991631799164, 0.6333333333333333, 0.6348547717842323, 0.6322314049586777, 0.6296296296296297, 0.6311475409836066, 0.6326530612244898, 0.6300813008130082, 0.631578947368421, 0.6330645161290323, 0.6345381526104418, 0.636, 0.6334661354581673, 0.6309523809523809, 0.6324110671936759, 0.6299212598425197, 0.6313725490196078, 0.6328125, 0.6342412451361867, 0.6356589147286822, 0.637065637065637, 0.6384615384615384, 0.6360153256704981, 0.6374045801526718, 0.6387832699619772, 0.6401515151515151, 0.6377358490566037, 0.6353383458646616, 0.6367041198501873, 0.6343283582089553, 0.6356877323420075, 0.6370370370370371, 0.6346863468634686, 0.6360294117647058, 0.6336996336996337, 0.635036496350365, 0.6363636363636364, 0.6376811594202898, 0.6353790613718412, 0.6366906474820144, 0.6379928315412187, 0.6392857142857142, 0.6405693950177936, 0.6418439716312057, 0.6431095406360424, 0.6443661971830986, 0.6456140350877193, 0.6468531468531469, 0.6480836236933798, 0.6458333333333334, 0.643598615916955, 0.6448275862068965, 0.6460481099656358, 0.6472602739726028, 0.6484641638225256, 0.6462585034013606, 0.6440677966101694, 0.6452702702702703, 0.6430976430976431, 0.6409395973154363, 0.6421404682274248, 0.6433333333333333, 0.6411960132890365, 0.6423841059602649, 0.6435643564356436, 0.6447368421052632, 0.6459016393442623, 0.6437908496732027, 0.6416938110749185, 0.6428571428571429, 0.6440129449838188, 0.6419354838709678, 0.639871382636656, 0.6378205128205128, 0.6357827476038339, 0.6369426751592356, 0.638095238095238, 0.6392405063291139, 0.637223974763407, 0.6383647798742138, 0.6363636363636364, 0.6375, 0.6386292834890965, 0.639751552795031, 0.6408668730650154, 0.6419753086419753, 0.64, 0.6411042944785276, 0.6422018348623854, 0.6432926829268293, 0.6443768996960486, 0.6454545454545455, 0.6465256797583081, 0.6475903614457831, 0.6486486486486487, 0.6497005988023952, 0.6507462686567164, 0.6517857142857143, 0.6498516320474778, 0.6479289940828402, 0.6489675516224189, 0.6470588235294118, 0.6480938416422287, 0.6491228070175439, 0.6501457725947521, 0.6511627906976745, 0.6521739130434783, 0.653179190751445, 0.654178674351585, 0.6551724137931034, 0.6561604584527221, 0.6542857142857142, 0.6552706552706553, 0.65625, 0.6572237960339944, 0.655367231638418, 0.6563380281690141, 0.6573033707865169, 0.6582633053221288, 0.659217877094972, 0.6573816155988857, 0.6583333333333333, 0.6565096952908587, 0.6574585635359116, 0.6556473829201102, 0.6565934065934066, 0.6547945205479452, 0.6557377049180327, 0.6566757493188011, 0.657608695652174, 0.6585365853658537, 0.6567567567567567, 0.6549865229110512, 0.6559139784946236, 0.6541554959785523, 0.6550802139037433, 0.656, 0.6569148936170213, 0.6578249336870027, 0.6587301587301587, 0.6596306068601583, 0.6578947368421053, 0.6561679790026247, 0.6570680628272252, 0.6579634464751958, 0.65625, 0.6545454545454545, 0.6528497409326425, 0.6537467700258398, 0.654639175257732, 0.6555269922879178, 0.6538461538461539, 0.6521739130434783, 0.6530612244897959, 0.6539440203562341, 0.6522842639593909, 0.6506329113924051, 0.6515151515151515, 0.6498740554156172, 0.6507537688442211, 0.6491228070175439, 0.65, 0.6483790523690773, 0.6492537313432836, 0.6501240694789082, 0.650990099009901, 0.6518518518518519, 0.6502463054187192, 0.6511056511056511, 0.6495098039215687, 0.6479217603911981, 0.6487804878048781, 0.6472019464720195, 0.6456310679611651, 0.6464891041162227, 0.6473429951690821, 0.6481927710843374, 0.6490384615384616, 0.6474820143884892, 0.645933014354067, 0.6467780429594272, 0.6476190476190476, 0.6484560570071259, 0.6492890995260664, 0.6501182033096927, 0.6509433962264151, 0.6494117647058824, 0.6502347417840375, 0.6487119437939111, 0.647196261682243, 0.6480186480186481, 0.6488372093023256, 0.6473317865429234, 0.6481481481481481, 0.648960739030023, 0.6497695852534562, 0.6482758620689655, 0.6490825688073395, 0.6475972540045767, 0.6461187214611872, 0.6469248291571754, 0.6454545454545455, 0.6462585034013606, 0.6470588235294118, 0.6478555304740407, 0.6486486486486487, 0.6494382022471911, 0.647982062780269, 0.6465324384787472, 0.6473214285714286, 0.6481069042316259, 0.6488888888888888, 0.647450110864745, 0.6460176991150443, 0.6467991169977925, 0.6475770925110133, 0.6483516483516484, 0.6491228070175439, 0.649890590809628, 0.648471615720524, 0.6492374727668845, 0.65, 0.648590021691974, 0.6493506493506493, 0.6501079913606912, 0.6508620689655172, 0.6494623655913978, 0.6502145922746781, 0.6509635974304069, 0.6517094017094017, 0.652452025586354, 0.6531914893617021, 0.6539278131634819, 0.6546610169491526, 0.6553911205073996, 0.6561181434599156, 0.6568421052631579, 0.657563025210084, 0.6561844863731656, 0.6569037656903766, 0.6576200417536534, 0.6583333333333333, 0.659043659043659, 0.6597510373443983, 0.660455486542443, 0.6611570247933884, 0.6618556701030928, 0.6604938271604939, 0.6611909650924025, 0.6618852459016393, 0.6625766871165644, 0.6612244897959184, 0.6619144602851323, 0.6626016260162602, 0.6632860040567952, 0.6639676113360324, 0.6626262626262627, 0.6612903225806451, 0.6619718309859155, 0.6626506024096386, 0.6633266533066132, 0.662, 0.6626746506986028, 0.6633466135458167, 0.6640159045725647, 0.6646825396825397, 0.6653465346534654, 0.66600790513834, 0.6666666666666666, 0.6673228346456693, 0.6679764243614931, 0.6686274509803921, 0.6673189823874756, 0.66796875, 0.6686159844054581, 0.669260700389105, 0.6699029126213593, 0.6705426356589147, 0.6711798839458414, 0.6718146718146718, 0.6724470134874759, 0.6711538461538461, 0.6717850287907869, 0.6704980842911877, 0.6711281070745698, 0.6698473282442748, 0.6685714285714286, 0.6673003802281369, 0.6679316888045541, 0.6685606060606061, 0.6691871455576559, 0.6698113207547169, 0.6704331450094162, 0.6710526315789473, 0.6697936210131332, 0.6685393258426966, 0.6691588785046729, 0.6697761194029851, 0.6685288640595903, 0.6691449814126395, 0.6679035250463822, 0.6685185185185185, 0.6691312384473198, 0.6678966789667896, 0.6666666666666666, 0.6654411764705882, 0.6660550458715596, 0.6666666666666666, 0.6654478976234004, 0.666058394160584, 0.6648451730418944, 0.6636363636363637, 0.6642468239564429, 0.6648550724637681, 0.6654611211573237, 0.6660649819494585, 0.6666666666666666, 0.6672661870503597, 0.6660682226211849, 0.6648745519713262, 0.6654740608228981, 0.6660714285714285, 0.6648841354723708, 0.6654804270462633, 0.6660746003552398, 0.6648936170212766, 0.6637168141592921, 0.6643109540636042, 0.6631393298059964, 0.6637323943661971, 0.6625659050966608, 0.6631578947368421, 0.6619964973730298, 0.6625874125874126, 0.6614310645724258, 0.6602787456445993, 0.6608695652173913, 0.6597222222222222, 0.6603119584055459, 0.6608996539792388, 0.6597582037996546, 0.6586206896551724, 0.6592082616179001, 0.6580756013745704, 0.6586620926243568, 0.6592465753424658, 0.6581196581196581, 0.658703071672355, 0.6592844974446337, 0.6598639455782312, 0.6604414261460102, 0.6610169491525424, 0.6615905245346869, 0.660472972972973, 0.6593591905564924, 0.6582491582491582, 0.6571428571428571, 0.6577181208053692, 0.6582914572864321, 0.6588628762541806, 0.659432387312187, 0.6583333333333333, 0.6589018302828619, 0.659468438538206, 0.6583747927031509, 0.6589403973509934, 0.6595041322314049, 0.6600660066006601, 0.6589785831960461, 0.6578947368421053, 0.6584564860426929, 0.659016393442623, 0.6595744680851063, 0.6601307189542484, 0.6606851549755302, 0.6596091205211726, 0.6601626016260163, 0.6607142857142857, 0.6612641815235009, 0.6618122977346278, 0.6623586429725363, 0.6612903225806451, 0.6602254428341385, 0.6607717041800643, 0.6613162118780096, 0.6602564102564102, 0.6608, 0.6613418530351438, 0.6602870813397129, 0.660828025477707, 0.6613672496025437, 0.6619047619047619, 0.6624405705229794, 0.6613924050632911, 0.6619273301737757, 0.6624605678233438, 0.662992125984252, 0.6619496855345912, 0.6624803767660911, 0.6614420062695925, 0.6619718309859155, 0.6609375, 0.6614664586583463, 0.660436137071651, 0.6609642301710731, 0.6599378881987578, 0.6604651162790698, 0.6594427244582043, 0.6599690880989181, 0.6604938271604939, 0.6610169491525424, 0.66, 0.6605222734254992, 0.6595092024539877, 0.6600306278713629, 0.6605504587155964, 0.6610687022900763, 0.6600609756097561, 0.6605783866057838, 0.6610942249240122, 0.6600910470409712, 0.6606060606060606, 0.659606656580938, 0.6601208459214502, 0.6606334841628959, 0.661144578313253, 0.6616541353383458, 0.6621621621621622, 0.6611694152923538, 0.6616766467065869, 0.6606875934230194, 0.6611940298507463, 0.6616989567809239, 0.6622023809523809, 0.662704309063893, 0.6632047477744807, 0.6622222222222223, 0.6627218934911243, 0.6632200886262924, 0.6637168141592921, 0.6642120765832106, 0.663235294117647, 0.6637298091042585, 0.6642228739002932, 0.664714494875549, 0.6637426900584795, 0.6642335766423357, 0.6632653061224489, 0.6637554585152838, 0.6627906976744186, 0.6618287373004355, 0.6623188405797101, 0.662807525325615, 0.6632947976878613, 0.6623376623376623, 0.6628242074927954, 0.6618705035971223, 0.6623563218390804, 0.6614060258249641, 0.660458452722063, 0.6609442060085837, 0.6614285714285715, 0.6619115549215406, 0.6623931623931624, 0.662873399715505, 0.6633522727272727, 0.6638297872340425, 0.6628895184135978, 0.6633663366336634, 0.6624293785310734, 0.6629055007052186, 0.6633802816901408, 0.6638537271448663, 0.6629213483146067, 0.6633941093969145, 0.6638655462184874, 0.6643356643356644, 0.664804469273743, 0.6652719665271967, 0.6657381615598886, 0.6662030598052852, 0.6652777777777777, 0.665742024965326, 0.6662049861495845, 0.665283540802213, 0.6657458563535912, 0.6648275862068965, 0.6639118457300276, 0.6629986244841816, 0.6620879120879121, 0.6625514403292181, 0.6616438356164384, 0.6607387140902873, 0.6598360655737705, 0.660300136425648, 0.6607629427792916, 0.6598639455782312, 0.6603260869565217, 0.66078697421981, 0.6612466124661247, 0.6617050067658998, 0.6608108108108108, 0.6612685560053981, 0.660377358490566, 0.6594885598923284, 0.6586021505376344, 0.6590604026845638, 0.6595174262734584, 0.6599732262382865, 0.660427807486631, 0.6608811748998665, 0.6613333333333333, 0.6617842876165113, 0.6622340425531915, 0.6626826029216467, 0.6618037135278515, 0.6622516556291391, 0.6613756613756614, 0.6605019815059445, 0.6609498680738787, 0.6600790513833992, 0.6592105263157895, 0.6583442838370565, 0.6587926509186351, 0.6592398427260813, 0.6596858638743456, 0.6601307189542484, 0.660574412532637, 0.6610169491525424, 0.6614583333333334, 0.6618985695708712, 0.6610389610389611, 0.6601815823605707, 0.6606217616580311, 0.6610608020698577, 0.661498708010336, 0.6606451612903226, 0.6597938144329897, 0.6589446589446589, 0.6593830334190232, 0.6598202824133504, 0.6602564102564102, 0.6594110115236875, 0.659846547314578, 0.6602809706257982, 0.6594387755102041, 0.6585987261146496, 0.6577608142493638, 0.6581956797966964, 0.6586294416243654, 0.6590621039290241, 0.6582278481012658, 0.6586599241466498, 0.6590909090909091, 0.6595208070617906, 0.6586901763224181, 0.6591194968553459, 0.6595477386934674, 0.6599749058971142, 0.6591478696741855, 0.6595744680851063, 0.65875, 0.6579275905118602, 0.6583541147132169, 0.6587795765877957, 0.6579601990049752, 0.6583850931677019, 0.6588089330024814, 0.6579925650557621, 0.6584158415841584, 0.657601977750309, 0.6567901234567901, 0.657213316892725, 0.6576354679802956, 0.6568265682656826, 0.6572481572481572, 0.6576687116564417, 0.6580882352941176, 0.6585067319461444, 0.6577017114914425, 0.6581196581196581, 0.6585365853658537, 0.658952496954933, 0.6581508515815085, 0.6573511543134872, 0.6577669902912622, 0.6581818181818182, 0.6585956416464891, 0.6590084643288996, 0.6594202898550725, 0.6586248492159228, 0.6590361445783133, 0.6594464500601684, 0.6598557692307693, 0.6602641056422569, 0.6594724220623501, 0.6598802395209581, 0.6590909090909091, 0.6583034647550776, 0.6587112171837709, 0.6579261025029798, 0.6571428571428571, 0.6575505350772889, 0.6579572446555819, 0.6571767497034401, 0.6563981042654028, 0.6568047337278107, 0.6572104018912529, 0.6564344746162928, 0.6556603773584906, 0.6560659599528857, 0.6564705882352941, 0.6568742655699178, 0.6572769953051644, 0.6565064478311841, 0.6569086651053864, 0.6573099415204678, 0.6565420560747663, 0.6569428238039673, 0.6573426573426573, 0.6577415599534342, 0.6581395348837209, 0.6573751451800233, 0.6566125290023201, 0.657010428736964, 0.6574074074074074, 0.6578034682080924, 0.6581986143187067, 0.6585928489042676, 0.6578341013824884, 0.6582278481012658, 0.6586206896551724, 0.6590126291618829, 0.6594036697247706, 0.6586483390607102, 0.6578947368421053, 0.6582857142857143, 0.658675799086758, 0.6579247434435576, 0.6583143507972665, 0.6575654152445961, 0.6568181818181819, 0.6572077185017026, 0.6575963718820862, 0.6579841449603624, 0.6583710407239819, 0.6587570621468927, 0.6591422121896162, 0.6595264937993236, 0.6599099099099099, 0.6602924634420697, 0.6606741573033708, 0.6610549943883277, 0.6614349775784754, 0.6606942889137738, 0.6610738255033557, 0.6614525139664804, 0.6607142857142857, 0.6610925306577481, 0.6603563474387528, 0.660734149054505, 0.66, 0.660377358490566, 0.6596452328159645, 0.6589147286821705, 0.6581858407079646, 0.6574585635359116, 0.6578366445916115, 0.6571113561190739, 0.6574889867841409, 0.6578657865786579, 0.6582417582417582, 0.6586169045005489, 0.6589912280701754, 0.6593647316538883, 0.6597374179431073, 0.659016393442623, 0.6593886462882096, 0.6597600872410033, 0.6601307189542484, 0.6594124047878128, 0.658695652173913, 0.6579804560260586, 0.658351409978308, 0.6576381365113759, 0.6569264069264069, 0.6572972972972972, 0.6576673866090713, 0.656957928802589, 0.65625, 0.6566200215285253, 0.656989247311828, 0.6573576799140709, 0.657725321888412, 0.6580921757770632, 0.658458244111349, 0.6588235294117647, 0.6591880341880342, 0.6595517609391676, 0.6588486140724946, 0.65814696485623, 0.6585106382978724, 0.6578108395324124, 0.6581740976645435, 0.6574761399787911, 0.6578389830508474, 0.6582010582010582, 0.6585623678646935, 0.6578669482576558, 0.6582278481012658, 0.6585879873551106, 0.6589473684210526, 0.6593059936908517, 0.6596638655462185, 0.6600209863588667, 0.660377358490566, 0.6596858638743456, 0.6600418410041841, 0.6603970741901777, 0.6607515657620042, 0.6600625651720542, 0.6604166666666667, 0.6607700312174818, 0.66008316008316, 0.660436137071651, 0.6597510373443983, 0.6601036269430052, 0.6594202898550725, 0.6597724922440538, 0.6590909090909091, 0.6594427244582043, 0.6597938144329897, 0.659114315139032, 0.6594650205761317, 0.6598150051387461, 0.660164271047228, 0.6605128205128206, 0.6608606557377049, 0.661207778915046, 0.6605316973415133, 0.6598569969356486, 0.6602040816326531, 0.6595310907237513, 0.659877800407332, 0.6602238046795524, 0.6605691056910569, 0.6609137055837564, 0.6612576064908722, 0.6616008105369807, 0.6619433198380567, 0.6622851365015167, 0.6626262626262627, 0.6629667003027245, 0.6622983870967742, 0.6626384692849949, 0.6629778672032193, 0.6623115577889447, 0.6616465863453815, 0.6619859578736209, 0.6613226452905812, 0.6616616616616616, 0.662]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [0.4, 0.3, 0.5, 0.7, 0.2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "total_rewards_per_trial = []\n",
        "cumulative_mean_rewards_per_trial = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "    cumulative_mean_rewards = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "        cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    cumulative_mean_rewards_per_trial.append(cumulative_mean_rewards)\n",
        "\n",
        "average_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "# Calculate the average cumulative mean rewards after all trials\n",
        "average_cumulative_mean_rewards = np.mean(cumulative_mean_rewards_per_trial, axis=0)\n",
        "\n",
        "print(\"Average Total Reward across Trials:\", average_total_reward)\n",
        "print(\"Average Cumulative Mean Rewards across Trials:\", average_cumulative_mean_rewards)"
      ],
      "metadata": {
        "id": "cxh3ND5TZbsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    def choose_arm(epsilon, estimates):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(num_arms)\n",
        "        else:\n",
        "            return np.argmax(estimates)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm:\", estimated_mean_rewards_per_arm)"
      ],
      "metadata": {
        "id": "qvWwem5DZfPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)\n"
      ],
      "metadata": {
        "id": "44OYc-AnZiOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Greedy approach: Always choose the arm with the highest estimated mean reward\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbi8oDmsZktZ",
        "outputId": "2d3a90c1-c17c-4c39-e657-dc51f630ae6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (Greedy Approach): [0.36090429 0.         0.         0.         0.        ]\n",
            "Mean Total Reward across Trials: 571.186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "\n",
        "            chosen_arm = np.random.choice(num_arms)\n",
        "        else:\n",
        "\n",
        "            chosen_arm = np.argmax(estimates)\n",
        "\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (ε-Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UBEV_o5ZnOU",
        "outputId": "c15bd4db-56c3-42c1-9f69-724f21f5604e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (ε-Greedy Approach): [0.3598898  0.27063718 0.45053953 0.6319367  0.17962416]\n",
            "Mean Total Reward across Trials: 599.019\n"
          ]
        }
      ]
    }
  ]
}